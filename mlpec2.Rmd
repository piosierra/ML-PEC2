---
title: "PEC2- ML"
author: "Pío Alberto Sierra Rodríguez"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
params:
  data_file: peptidos.csv
  training_split: 67
  seed.split: 123
  seed.clsfier: 1234567
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include = FALSE}
if(!(require(ggseqlogo)))
  install.packages("ggseqlogo")
if(!(require(caret)))
  install.packages("caret")
if(!(require(e1071)))
  install.packages("e1071")
if(!(require(neuralnet)))
  install.packages("NeuralNetTools)")
if(!(require(NeuralNetTools)))
  install.packages("NeuralNetTools")
if(!(require(kernlab)))
  install.packages("kernlab")
if(!(require(class)))
  install.packages("class")
if(!(require(gmodels)))
  install.packages("gmodels")
if(!(require(ROCR)))
  install.packages("ROCR")
if(!(require(caret)))
  install.packages("caret")
if(!(require(e1071)))
  install.packages("e1071")
```

***  
**Parámetros:**  
**data_file**: *ruta completa al archivo de péptidos*  
**training_splt**: *porcentaje a dedicar al conjunto de training [0,100]*
*** 

# Algoritmo Red Neuronal Artificial

El algoritmo de Red Neuronal Artificial con retropropagación está inspirado en el funcionamiento de las redes neuronales biológicas, pero con un menor número de neuronas y capas. Funciona iterando repetidamente dos procesos. En uno se obtiene una salida y en otro se propaga el error de esa salida hacia atrás para corregir los pesos de de cada neurona, mediante la técnica de gradiente descendente.

Fortalezas | Debilidades
-----|------
Se puede adaptar a problemas de clasificación o predicción numéricos  | Muy costosa computacionalmente y lenta durante el aprendizaje, especialmente si la topología de la red es compleja.  
Capaz de modelizar estructuras más complejas que la mayoría de algoritmos | Es fácil caer en problemas de overfitting con esta técnica.  
Hace pocas presunciones sobre las relaciones internas de los datos | El resultado en un modelo de caja negra que es muy difícil, si no imposible, de interpretar. 

# Algoritmo Support Vector Machine

El algoritmo SVN consiste en la creacción de hiperplanos en el espacio definido por los valores de las características que creen particiones en las que clasificar los datos.  

Ventajas| Inconvenientes
-----|------
Pueden ser usados tanto para clasificación como para predicción numérica | Encontrar el mejor modelo exige probar con distintos kernels y parámetros.  
No se ve muy afectado por ruido en los datos ni es susceptible de sobreajuste. | El entrenamiento puede ser lento, sobre todo cuando se trata de datos con un gran número de características u observaciones.  
Puede resultar más sencillo de utilizar que las redes neuronales, debido a la existencia de varios algoritmos de SVN bien mantenidos. | El resultado es un complejo modelo de caja negra que resulta a menudo imposible de interpretar.  
Es bastante popular debido a su precisión y al haber resultado vencedor en varias competiciones de data mining. | -  

El algoritmo consiste en identificar los Support Vectors, las observaciones de cada clase que están más cerca del Hyperplano de margen máximo (MMH). Se demuestra que estos puntos bastan para definir el hiperplano. Para el caso de datos que no son linealmente separables, se utiliza una variable slack que define un margen al otro lado del hiperplano al que es aceptable tener alguna observación.  

Otra ventaja de SVN es la posiblidad de añadir más dimensiones a las observaciones para así hacer visibles (lineales) relaciones entre las observaciones.  

# Lectura de datos

```{r}
peptidos <- read.csv2(params$data_file)
table(peptidos$label)
ggseqlogo(peptidos[which(peptidos$label=="SB"),]$sequence)
ggseqlogo(peptidos[which(peptidos$label=="NB"),]$sequence)
```

Tenemos un `r 100*sum(peptidos$label=="SB")/length(peptidos$label)`% de peptidos con interacción.  

Podemos observar que los péptidos que tienen interacción presentan una mayor frecuencia muy marcada de aminoácidos hidrófobos en la segunda y última posición de la cadena.  

# Codificación "one-hot"

```{r}
aminoacidos <-c('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',
                'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y')
one_hot <- function(data) {
    unlist(lapply(strsplit(data, split="")[[1]], 
                  function(x) {as.integer(aminoacidos == x)}), 
           recursive = FALSE)
}
oh_data <- data.frame(t(data.frame(lapply(peptidos$sequence, one_hot))))
rownames(oh_data) <- peptidos$sequence  
```

# Clasificador de red neuronal artificial

Creamos variables binarias en lugar de usar la variable factor `label`.   
```{r}
oh_data$SB <- as.integer(peptidos$label=="SB")
oh_data$NB <- as.integer(peptidos$label=="NB")
n <- nrow(oh_data)
```
Ahora creamos los conjuntos entrenamiento y de prueba. Se realiza una extracción de los datos *aleatoriamente* de `r params$training_split`% de todas las observaciones, `r floor(params$training_split*n/100)`, para entrenar al modelo (training) y del resto `r n - floor(params$training_split*n/100)` para evaluarlo (test).
```{r}
set.seed(params$seed.split)
smp_size <-  floor(n * params$training_split/100)
train_ind <- sample(seq_len(n), size = smp_size)
data_train <- oh_data[train_ind,]
data_test <- oh_data[-train_ind,]
```

```{r}
set.seed(params$seed.clsfier)
xnam <- names(oh_data[1:180])
fmla <- as.formula(paste("SB+NB ~ ", paste(xnam, collapse= "+")))

data_model_1 <- neuralnet(fmla,
                          data = data_train,
                          hidden=1,linear.output=FALSE)
plot(data_model_1, rep='best')
model_results_1 <- compute(data_model_1, data_test[,1:180])$net.result

maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(model_results_1, 1, maxidx)
prediction <- c('SB', 'NB')[idx]
res <- table(prediction, peptidos$label[-train_ind] )

(cmatrix1 <- confusionMatrix(res,positive="SB"))
```